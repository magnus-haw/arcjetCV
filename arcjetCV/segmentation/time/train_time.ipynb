{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9BQ3HlgbJGj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import optimize, integrate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6MMhiSNfbcn"
      },
      "outputs": [],
      "source": [
        "def recursive_fit_piecewise(x,y, err_thresh, level=0, slope=10.):\n",
        "    \"\"\" Recursive top-down algorithm for piece-wise linear\n",
        "        fitting. Consecutive splits of domain based on error\n",
        "        threshold.\n",
        "\n",
        "    Args:\n",
        "        x (1D array): independent variable (float)\n",
        "        y (1D array): dependent variable (float)\n",
        "        err_thresh (float): rms err threshold for curve segment\n",
        "\n",
        "    Returns:\n",
        "        elbow points: list of (x,y) points\n",
        "    \"\"\"\n",
        "    p, cov, left_rms, right_rms = fit_piecewise(x,y)\n",
        "    lpoints, rpoints = [], []\n",
        "    if left_rms > err_thresh:\n",
        "        lx, ly = x[x<=p[0]], y[x<=p[0]]\n",
        "        if len(lx) > 10:\n",
        "            lpoints = recursive_fit_piecewise(lx,ly, err_thresh, level=level+1)\n",
        "    if right_rms > err_thresh:\n",
        "        rx, ry = x[x>=p[0]], y[x>=p[0]]\n",
        "        if len(rx) > 10:\n",
        "            rpoints = recursive_fit_piecewise(rx,ry, err_thresh, level=level+1)\n",
        "\n",
        "    if level ==0:\n",
        "        lpoints = [(x[0],y[0])]+lpoints\n",
        "        rpoints = rpoints + [(x[-1],y[-1])]\n",
        "    if left_rms < err_thresh or right_rms < err_thresh:\n",
        "        ret = lpoints + [(p[0],p[1])] + rpoints\n",
        "    else:\n",
        "        ret = lpoints + rpoints\n",
        "\n",
        "    if level == 0:\n",
        "        ret = check_segments(x,y,ret,thresh=err_thresh)\n",
        "        ret = merge_similar_segments(ret, slope=slope)\n",
        "        ret = np.array(ret)\n",
        "\n",
        "    return ret\n",
        "\n",
        "def check_segments(x,y,pts,thresh=7.5):\n",
        "    new_pts =[pts[0]]\n",
        "    for i in range(1,len(pts)):\n",
        "        x0,y0 = pts[i-1]\n",
        "        x1,y1 = pts[i]\n",
        "\n",
        "        inds = (x>=x0)*(x<=x1)\n",
        "        xp,yp = x[inds],y[inds]\n",
        "\n",
        "        yi = (y1-y0)*(xp-x0) + y0\n",
        "        rms = np.sqrt(np.mean((yi-yp)**2))\n",
        "        if rms > thresh:\n",
        "            p, cov, left_rms, right_rms = fit_piecewise(xp,yp)\n",
        "            new_pts.append((p[0],p[1]))\n",
        "        new_pts.append(pts[i])\n",
        "    return new_pts\n",
        "\n",
        "def piecewise_linear(x, x0, y0, k1, k2):\n",
        "    x = np.array(x, dtype=np.float)\n",
        "    return np.piecewise(x, [x <= x0, x>x0],\n",
        "                        [lambda x:k1*x + y0-k1*x0, lambda x:k2*x + y0-k2*x0])\n",
        "\n",
        "def linear(x, x0, y0, k1):\n",
        "    return lambda x:k1*x + y0-k1*x0\n",
        "\n",
        "def fit_linear(x,y):\n",
        "    ymax= max(y.min()+1,y.max())\n",
        "    p, cov = optimize.curve_fit(linear, x, y, p0=(np.mean(x),np.mean(y),0.),\n",
        "                                bounds=([x.min(),y.min(),-np.inf],\n",
        "                                        [x.max(),ymax, np.inf]))\n",
        "    rms = np.sqrt( np.mean((linear(x,*p) - y)**2) )\n",
        "    return p, cov, rms\n",
        "\n",
        "def fit_piecewise(x,y):\n",
        "    \"\"\"Fits 2-piece linear function to data\n",
        "\n",
        "    Args:\n",
        "        x (1D array): independent variable (float)\n",
        "        y (1D array): dependent variable (float)\n",
        "\n",
        "    Returns:\n",
        "        p: parameter tuple (x0,y0,k1,k2)\n",
        "        cov: covariance matrix\n",
        "        left_rms: left curve rms\n",
        "        right_rms: right curve rms\n",
        "    \"\"\"\n",
        "    ymax= max(y.min()+1,y.max())\n",
        "    p, cov = optimize.curve_fit(piecewise_linear, x, y, p0=(np.mean(x),np.mean(y),0.,0.),\n",
        "                                bounds=([x.min(),y.min(),-np.inf, -np.inf],\n",
        "                                        [x.max(),ymax, np.inf, np.inf]))\n",
        "    left_rms = np.sqrt( np.mean((piecewise_linear(x[x<=p[0]],*p) - y[x<=p[0]])**2) )\n",
        "    right_rms = np.sqrt( np.mean((piecewise_linear(x[x>p[0]],*p) - y[x>p[0]])**2) )\n",
        "    return p, cov, left_rms, right_rms\n",
        "\n",
        "def smooth(x,window_len=11,window='hanning'):\n",
        "    \"\"\"smooth the data using a window with requested size.\n",
        "\n",
        "    This method is based on the convolution of a scaled window with the signal.\n",
        "    The signal is prepared by introducing reflected copies of the signal\n",
        "    (with the window size) in both ends so that transient parts are minimized\n",
        "    in the begining and end part of the output signal.\n",
        "\n",
        "    input:\n",
        "        x: the input signal\n",
        "        window_len: the dimension of the smoothing window; should be an odd integer\n",
        "        window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\n",
        "            flat window will produce a moving average smoothing.\n",
        "\n",
        "    output:\n",
        "        the smoothed signal\n",
        "\n",
        "    example:\n",
        "\n",
        "    t=linspace(-2,2,0.1)\n",
        "    x=sin(t)+randn(len(t))*0.1\n",
        "    y=smooth(x)\n",
        "\n",
        "    see also:\n",
        "\n",
        "    numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve\n",
        "    scipy.signal.lfilter\n",
        "\n",
        "    TODO: the window parameter could be the window itself if an array instead of a string\n",
        "    NOTE: length(output) != length(input), to correct this: return y[(window_len/2-1):-(window_len/2)] instead of just y.\n",
        "    \"\"\"\n",
        "\n",
        "    x = np.array(x)\n",
        "    if x.ndim != 1:\n",
        "        raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n",
        "\n",
        "    if x.size < window_len:\n",
        "        raise ValueError(\"Input vector needs to be bigger than window size.\")\n",
        "\n",
        "    if window_len<3:\n",
        "        return x\n",
        "\n",
        "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
        "        raise ValueError(\"Window is one of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\")\n",
        "\n",
        "    #s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
        "    #print(len(s))\n",
        "    if window == 'flat': #moving average\n",
        "        w=np.ones(window_len,'d')\n",
        "    else:\n",
        "        w=eval('np.'+window+'(window_len)')\n",
        "\n",
        "    N = window_len\n",
        "    spad = np.pad(x, (N//2, N-1-N//2), mode='edge')\n",
        "    ret = np.convolve(w/w.sum(),spad,mode='valid')\n",
        "    #ret = y[int(window_len/2)-1:-int(window_len/2)-1]\n",
        "\n",
        "    assert len(ret) == len(x)\n",
        "    return ret\n",
        "\n",
        "def merge_similar_segments(pts, slope=10.):\n",
        "    ### Merge adjacent segements with similar slopes\n",
        "    clean_pts = [pts[0]]\n",
        "    for i in range(1,len(pts)-1):\n",
        "        dx1,dx2 = pts[i][0] - pts[i-1][0], pts[i+1][0] - pts[i][0]\n",
        "        dy1,dy2 = pts[i][1] - pts[i-1][1], pts[i+1][1] - pts[i][1]\n",
        "\n",
        "        m1, m2 = dy1/dx1, dy2/dx2\n",
        "\n",
        "        if abs(m1-m2)>slope:\n",
        "            clean_pts.append(pts[i])\n",
        "    clean_pts.append(pts[-1])\n",
        "    return clean_pts\n",
        "\n",
        "def remove_small_segments(clean_pts):\n",
        "    ### Eliminate small segments between larger segments\n",
        "    if len(clean_pts) >= 5:\n",
        "        extra_clean = [clean_pts[0]]\n",
        "        pts = np.array(clean_pts)\n",
        "        dp = np.diff(pts,axis=0)\n",
        "        xnorm = pts[:,0].max() - pts[:,0].min()\n",
        "        ynorm = pts[:,1].max() - pts[:,1].min()\n",
        "        dp[:,0] /= xnorm\n",
        "        dp[:,1] /= ynorm\n",
        "\n",
        "        i=1\n",
        "        while i < len(clean_pts)-2:\n",
        "            dl1 = np.linalg.norm(dp[i-1])\n",
        "            dl2 = np.linalg.norm(dp[i])\n",
        "            dl3 = np.linalg.norm(dp[i+1])\n",
        "\n",
        "            if dl1/dl2 > 5 and dl3/dl2 > 5:\n",
        "                extra_clean.append(clean_pts[i+1])\n",
        "                i += 2\n",
        "            else:\n",
        "                extra_clean.append(clean_pts[i])\n",
        "                i += 1\n",
        "        extra_clean.append(clean_pts[-2])\n",
        "        extra_clean.append(clean_pts[-1])\n",
        "        return np.array(extra_clean)\n",
        "    else:\n",
        "        return np.array(clean_pts)\n",
        "\n",
        "def getOutlierMask(metric,threshold=2,method=\"stdev\"):\n",
        "    \"\"\"Returns mask for outliers in time series\n",
        "\n",
        "    Args:\n",
        "        metric (array): 1D numpy array, presumably time series\n",
        "        threshold (int, optional): Threshold value. Default is 2 for stdev.\n",
        "        method (str, optional): \"stdev\" or \"percent\". Defaults to \"stdev\".\n",
        "\n",
        "    Returns:\n",
        "        mask: numpy masked array\n",
        "    \"\"\"\n",
        "    mask = np.zeros_like(metric)\n",
        "    if len(metric) > 25:\n",
        "        smetric = smooth(metric)\n",
        "        diff = abs(smetric - metric)\n",
        "\n",
        "        if method==\"percent\":\n",
        "            rdiff = abs(diff/(metric + 1e-3))\n",
        "            mask += rdiff > threshold\n",
        "\n",
        "        if method==\"stdev\":\n",
        "            dmean = np.mean(diff)\n",
        "            dstd = np.std(diff)\n",
        "            mask += diff > dmean + threshold*dstd\n",
        "    else:\n",
        "        print(\"Input array len must be > 25\")\n",
        "    return mask\n",
        "\n",
        "def clean_series(x,y):\n",
        "    inds = ~np.isnan(y)\n",
        "    xn = x[inds]\n",
        "    yn = y[inds]\n",
        "    mask = getOutlierMask(yn)\n",
        "    ynn = yn[mask==0]\n",
        "    xnn = xn[mask==0]\n",
        "    return xnn.copy(),ynn.copy()\n",
        "\n",
        "def integral_estimator(xi,yi, thresh=40., slope=10.):\n",
        "    x,y = clean_series(xi,yi)\n",
        "    y_int = integrate.cumtrapz(y, x, initial=0)\n",
        "    pts = recursive_fit_piecewise(x,y_int,thresh,slope=slope)\n",
        "    pts = remove_small_segments(pts)\n",
        "    return np.array(pts)\n",
        "\n",
        "def infer_stable_portions(t,yi, fraction_thresh=0.1, buffer_dt=[10,2]):\n",
        "    wd = min(len(t)//20, 95)\n",
        "    #extract each condition segment\n",
        "    y = smooth(yi,window_len=wd)\n",
        "    # pick out value at halfway mark\n",
        "    n = len(t)//2\n",
        "    constval = np.nanmean(yi[n:n+20])\n",
        "    # find first point where value stabilizes within threshold (e.g. 10%)\n",
        "    dy,dymax = abs(y - constval), constval*fraction_thresh\n",
        "    ts = t[0:n]\n",
        "    unstable = ts[dy[0:n] > dymax]\n",
        "    try:\n",
        "        stable_start = unstable[-1] + buffer_dt[0]\n",
        "    except:\n",
        "        stable_start = ts[0] + buffer_dt[0]\n",
        "    # find last point where value stabilizes within threshold (e.g. 10%)\n",
        "    stable = t[dy < dymax]\n",
        "    try:\n",
        "        stable_end = stable[-1] - buffer_dt[1]\n",
        "    except:\n",
        "        stable_end = t[-1] - buffer_dt[1]\n",
        "\n",
        "    # plt.plot(t,dy)\n",
        "    # print(constval)\n",
        "    # plt.show()\n",
        "    return stable_start, stable_end, constval\n",
        "\n",
        "def infer_conditions(t,yi, thresh=40., minval=10.):\n",
        "    pts = integral_estimator(t,yi, thresh=thresh, slope=minval)\n",
        "    dt = np.gradient(pts[:,0])\n",
        "    dI = np.gradient(pts[:,1])\n",
        "    conditions = []\n",
        "    for i in range(0,len(dt)):\n",
        "        if dt[i] >5 and dI[i]/dt[i] > minval:\n",
        "            inds = (t>pts[i,0])*(t<pts[i+1,0])\n",
        "            if len(conditions) == 0:\n",
        "                buffer_dt = [10,3]\n",
        "            else:\n",
        "                buffer_dt = [2,3]\n",
        "            stable_start, stable_end, avg = infer_stable_portions(t[inds],yi[inds],buffer_dt=buffer_dt)\n",
        "            conditions.append({'value':avg, 'start':pts[i,0], 'end':pts[i+1,0],\n",
        "                               'stable_start':stable_start, 'stable_end':stable_end})\n",
        "    return conditions\n",
        "\n",
        "def get_intersection(a0,a1,b0,b1):\n",
        "    t0,t1 = max(a0,b0),min(a1,b1)\n",
        "    if t0 > t1:\n",
        "        return None, None\n",
        "    else:\n",
        "        return t0,t1\n",
        "\n",
        "def collate_conditions(condsI,condsF):\n",
        "    allconds = []\n",
        "    for icond in condsI:\n",
        "        for fcond in condsF:\n",
        "            t0,t1 = get_intersection(icond['start'],icond['end'],fcond['start'],fcond['end'])\n",
        "            ts0,ts1 = get_intersection(icond['stable_start'],icond['stable_end'],fcond['stable_start'],fcond['stable_end'])\n",
        "            ival,fval = icond['value'],fcond['value']\n",
        "            if t0 is not None:\n",
        "                allconds.append({'I_val':ival,'F_val':fval, 'start':t0, 'end':t1,\n",
        "                                 'stable_start':ts0, 'stable_end':ts1})\n",
        "    return allconds\n",
        "\n",
        "def get_ConditionInstance(cond_dict, ListInsts, thresh=.15):\n",
        "    score = np.zeros(len(ListInsts))\n",
        "    for i in range(0,len(ListInsts)):\n",
        "        cinst = ListInsts[i]\n",
        "        I = cinst.condition.current\n",
        "        F = cinst.condition.plasma_gas_flow\n",
        "        ierr = abs(cond_dict['I_val'] - I)/cond_dict['I_val']\n",
        "        ferr = abs(cond_dict['F_val']-F)/cond_dict['F_val']\n",
        "        score[i] = np.sqrt(ierr**2 + ferr**2)\n",
        "    if score.min() < thresh:\n",
        "        return ListInsts[int(np.argmin(score))]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def get_condition_match(cond_dict, cinst, thresh=.15):\n",
        "    I = cinst.condition.current\n",
        "    F = cinst.condition.plasma_gas_flow\n",
        "    ierr = abs(cond_dict['I_val'] - I)/cond_dict['I_val']\n",
        "    ferr = abs(cond_dict['F_val']-F)/cond_dict['F_val']\n",
        "    score = np.sqrt(ierr**2 + ferr**2)\n",
        "    if score < thresh:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zepY8_UEcUme"
      },
      "outputs": [],
      "source": [
        "class Conv1DNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Conv1DNet, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=7, stride=2, padding=3)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=7, stride=2, padding=3)\n",
        "        self.convtrans1 = nn.ConvTranspose1d(in_channels=16, out_channels=16, kernel_size=7, stride=2, padding=3, output_padding=1)\n",
        "        self.convtrans2 = nn.ConvTranspose1d(in_channels=16, out_channels=32, kernel_size=7, stride=2, padding=3, output_padding=1)\n",
        "        self.convtrans3 = nn.ConvTranspose1d(in_channels=32, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.convtrans1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.convtrans2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.convtrans3(x)\n",
        "        # Applying softmax on the last dimension (channels) to get probabilities\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La3r3JrNcctu"
      },
      "outputs": [],
      "source": [
        "### Artificial data generation function\n",
        "def gen_trace(amp =0.9, amp2=0.7, noise_amp=.015, n=500):\n",
        "    # Generate fake top hat data\n",
        "    rnd = sorted(np.random.random_sample((3,)))\n",
        "    start,middle,stop = rnd[0],rnd[1],rnd[2]\n",
        "    t = np.linspace(0, 1, n)\n",
        "    vals = np.zeros(n)\n",
        "    vals[t>start]= amp\n",
        "    vals[t>middle]= amp2\n",
        "    vals[t>stop] = 0\n",
        "    spikeamp = sorted(np.random.random_sample((1,)))[0]/2\n",
        "    if spikeamp> 0.2:\n",
        "        vals[abs(t-start)<0.015] = spikeamp\n",
        "    vals = smooth(vals)\n",
        "    noise = np.random.normal(0, noise_amp, (500))\n",
        "    raw = vals + noise\n",
        "    #newrnd\n",
        "\n",
        "    # label fake top hat data\n",
        "    labels = np.zeros(n)\n",
        "    labels[t>start]= 2\n",
        "    labels[t>stop] = 0\n",
        "    labels[abs(t-start)<.025] = 1\n",
        "    labels[abs(t-stop)<.025] = 1\n",
        "    labels[abs(t-middle)<.025] = 1\n",
        "\n",
        "    encoded_labels = np.zeros((n,3))\n",
        "    encoded_labels[:,0] = labels==0\n",
        "    encoded_labels[:,1] = labels==1\n",
        "    encoded_labels[:,2] = labels==2\n",
        "\n",
        "    return  raw, encoded_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEAO_Wu1clbC",
        "outputId": "8efe7380-951b-46e7-ef29-bc4bd94ea96e"
      },
      "outputs": [],
      "source": [
        "# Assuming gen_trace and smooth are defined as given and above\n",
        "# Generate data\n",
        "num_samples = 1000  # Number of samples to generate\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "for _ in range(num_samples):\n",
        "    raw, labels = gen_trace()\n",
        "    x_train.append(raw)\n",
        "    # Convert labels to single class labels if necessary. Here we choose the max label as class.\n",
        "    y_train.append(np.argmax(labels, axis=1))\n",
        "\n",
        "x_train = np.array(x_train, dtype=np.float32)\n",
        "y_train = np.array(y_train, dtype=np.long)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x_train = torch.from_numpy(x_train).long() # Shape: [batch, channels, length]\n",
        "y_train = torch.from_numpy(y_train).long()\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(x_train_tensor, y_train_tensor)  # Add channel dimension\n",
        "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "GoNMDxw3cJ-1",
        "outputId": "36b161bf-b66a-46c8-b43a-e37ded4b02e9"
      },
      "outputs": [],
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "model = Conv1DNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 40\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        inputs = inputs.unsqueeze(1)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "torch.save( model.state_dict(),\"time_seg.pth\")\n",
        "print('model saved')\n",
        "\n",
        "# Use the Colab file management to download\n",
        "# from google.colab import files\n",
        "# files.download('time_seg.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2j0FJaDfY2_"
      },
      "outputs": [],
      "source": [
        "# Generate a single test input sequence\n",
        "test_raw, test_labels = gen_trace()\n",
        "test_input = torch.tensor(test_raw, dtype=torch.float).unsqueeze(0).unsqueeze(1)  # Shape: [1, 1, 500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPeYFfBUfaMU",
        "outputId": "f093fb1c-9e59-41de-c747-19837e8780c8"
      },
      "outputs": [],
      "source": [
        "test_input = torch.tensor(test_raw, dtype=torch.float).unsqueeze(0).unsqueeze(1)  # Shape: [1, 1, 500]\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    test_output = model(test_input)\n",
        "    # # Convert logits to probabilities (optional)\n",
        "    probabilities = F.softmax(test_output, dim=1)\n",
        "    # Get the predicted class for each time step\n",
        "    predictions = torch.argmax(test_output, dim=1)#.squeeze()  # Shape: [500]\n",
        "    print(probabilities)\n",
        "# Flatten test_input for plotting\n",
        "test_input_flat = test_input.squeeze().cpu().numpy()\n",
        "\n",
        "# Convert probabilities to numpy for easy handling\n",
        "probabilities_np = predictions.squeeze().cpu().numpy()  # Shape: [num_classes, 500]\n",
        "\n",
        "# Time steps for x-axis\n",
        "time_steps = np.arange(test_input_flat.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "hnl2Fu4arVNZ",
        "outputId": "0847fc92-2ff3-4728-b5b3-b3d88663c2a9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Plot the input sequence\n",
        "plt.plot(time_steps, test_input_flat, label='Input Sequence', color='black', linewidth=1, alpha=0.7)\n",
        "\n",
        "# Plot prediction probabilities for each class\n",
        "class_colors = ['red', 'green', 'blue']  # Colors for classes 0, 1, 2\n",
        "class_labels = ['Class 0 Prob', 'Class 1 Prob', 'Class 2 Prob']  # Labels for classes 0, 1, 2\n",
        "\n",
        "for i, color in enumerate(class_colors):\n",
        "    plt.plot(time_steps, probabilities_np, label=class_labels[i], color=color, alpha=0.5)\n",
        "\n",
        "plt.title('Input Sequence and Class Prediction Probabilities')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Amplitude / Probability')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "q07etTo2ySV3",
        "outputId": "062e621a-9992-4772-afad-fe2532051d52"
      },
      "outputs": [],
      "source": [
        "# Selecting the first sequence in the batch for visualization\n",
        "sequence_probabilities = probabilities[0].cpu().numpy()  # Convert to numpy array for easy handling\n",
        "\n",
        "time_steps = np.arange(sequence_probabilities.shape[1])  # Assuming sequence_length is the size\n",
        "\n",
        "# Plotting the probabilities for each class\n",
        "plt.figure(figsize=(14, 4))\n",
        "for class_index in range(sequence_probabilities.shape[0]):  # Iterate over each class\n",
        "    plt.plot(time_steps, sequence_probabilities[class_index], label=f'Class {class_index}')\n",
        "\n",
        "plt.title('Class Probabilities over Time')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Probability')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
